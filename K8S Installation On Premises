NAT IP: 192.168.136.174
HOST ONLY: 
           Subnet: 10.0.0.0/8
            10.0.0.100  ===========> Master
            10.0.0.1 ================> Worker

POD IP Range: 172.16.0.0/16

STEP 1 [ON Master and Worker]
vi comman.sh

#!/bin/bash
#
# Common setup for ALL SERVERS (Control Plane and Nodes)
# will run on Ubuntu Server 22.04 LTS
# If you see some warning at certificates, RUN AGAIN

#set -euxo pipefail
apt-get update -y
# Variable Declaration

KUBERNETES_VERSION="1.29.0-1.1"

# disable swap
echo ""
echo  "\033[4mDisabling Swap Memory.\033[0m"
echo ""
sudo swapoff -a
sed -e '/swap/s/^/#/g' -i /etc/fstab

# Install CRI-O Runtime
echo ""
echo  "\033[4mInstalling CRI-O | Kubelet | Kubeadm | Kubectl.\033[0m"
echo ""
OS="xUbuntu_22.04"

VERSION="1.23"

# Create the .conf file to load the modules at bootup
cat <<EOF | sudo tee /etc/modules-load.d/crio.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

# Set up required sysctl params, these persist across reboots.
cat <<EOF | sudo tee /etc/sysctl.d/99-kubernetes-cri.conf
net.bridge.bridge-nf-call-iptables  = 1
net.ipv4.ip_forward                 = 1
net.bridge.bridge-nf-call-ip6tables = 1
EOF

sudo sysctl --system

#### NEW location for keyrings
echo
echo
echo "****************************************"
echo "INSTALLING CRI-O KUBEADM KUBECTL KUBELET"
echo "****************************************"
echo
echo
apt-get update -y
apt-get install -y software-properties-common curl
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key |
    gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /" |
    tee /etc/apt/sources.list.d/kubernetes.list

curl -fsSL https://pkgs.k8s.io/addons:/cri-o:/prerelease:/main/deb/Release.key |
    gpg --dearmor -o /etc/apt/keyrings/cri-o-apt-keyring.gpg
echo "deb [signed-by=/etc/apt/keyrings/cri-o-apt-keyring.gpg] https://pkgs.k8s.io/addons:/cri-o:/prerelease:/main/deb/ /" |
    tee /etc/apt/sources.list.d/cri-o.list

apt-get update -y
apt-get install -y cri-o cri-tools kubelet kubeadm kubectl




echo ""
echo  "\033[4mConfiguring CRI-O to use dockerhub.\033[0m"
echo ""
cat <<EOF | sudo tee /etc/crio/crio.conf
registries = [
"docker.io"
]
EOF

systemctl restart crio
echo ""
echo  "\033[4mCRI-O installed and configured.\033[0m"
echo ""
echo  "\033[4mSetting nameservers.\033[0m"
echo ""
#sudo systemctl stop systemd-resolved
#sudo rm /etc/resolv.conf
#create new /etc/resolv.conf with no loopback entry
#systemctl start systemd-resolved
#systemctl enable systemd-resolved


apt install resolvconf -y
systemctl start resolvconf.service
systemctl enable resolvconf.service
cat <<EOF | sudo tee /etc/resolvconf/resolv.conf.d/head
nameserver 8.8.8.8
nameserver 8.8.4.4
EOF

systemctl restart resolvconf.service
systemctl restart systemd-resolved.service

echo ""
echo "===="
echo "Generate token on manager using"
echo "==="
echo "kubeadm token create --print-join-command"
echo ""
echo ""

SAVE And EXIT

===================================================================================================================================================================
STEP: 2
#kubeadm config images pull  [On Master Only]

#kubeadm init --apiserver-advertise-address 10.0.0.100 --pod-network-cidr 172.16.0.0/16

Run Below command one by one on Master Node Only.
     mkdir -p $HOME/.kube
     sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
     sudo chown $(id -u):$(id -g) $HOME/.kube/config
===================================================================================================================================================================
STEP 3:
#vim calico.yaml  [On Master Only ]  =====>  Refere calico.yaml

#kubectl apply -f calico.yaml
====================================================================================================================================================================

STEP 4:
Create a Token On master:

#kubeadm token create --print-join-command
o/p:
    kubeadm join 10.0.0.100:6443 --token p4cg70.3d7irklgu1lsa34t --discovery-token-ca-cert-hash sha256:12392aae6319c7b579805230c428aa9139485ab1f29f56024d9e88ee613e1cf5


===========================================================================================================================================================================

STEP 5:

Run Below Command on Worker node to join in cluster

#kubeadm join 10.0.0.100:6443 --token p4cg70.3d7irklgu1lsa34t --discovery-token-ca-cert-hash sha256:12392aae6319c7b579805230c428aa9139485ab1f29f56024d9e88ee613e1cf5

=====================================================================================================================================================================
STEP 6:
Check Worker is sucessfully added or not OR Check how much number of Nodes present in Cluster  [ RUN Only On Master ]

#kubectl get nodes
NAME        STATUS   ROLES           AGE    VERSION
k8smaster   Ready    control-plane   133m   v1.29.15
worker      Ready    <none>          83m    v1.29.15


Bash Complition:
#source <(kubectl completion bash)
#echo "source <(kubectl completion bash)" >> ~/.bashrc


#kubectl -n kube-system get pods -o wide 
NAME                                       READY   STATUS    RESTARTS   AGE    IP                NODE        NOMINATED NODE   READINESS GATES
calico-kube-controllers-658d97c59c-fk6r6   1/1     Running   0          131m   172.16.16.129     k8smaster   <none>           <none>
calico-node-75xk7                          1/1     Running   0          86m    192.168.136.175   worker      <none>           <none>
calico-node-b2tp4                          1/1     Running   0          131m   192.168.136.174   k8smaster   <none>           <none>
coredns-76f75df574-lgltj                   1/1     Running   0          136m   172.16.16.131     k8smaster   <none>           <none>
coredns-76f75df574-t8hcf                   1/1     Running   0          136m   172.16.16.130     k8smaster   <none>           <none>
etcd-k8smaster                             1/1     Running   0          136m   192.168.136.174   k8smaster   <none>           <none>
kube-apiserver-k8smaster                   1/1     Running   0          136m   192.168.136.174   k8smaster   <none>           <none>
kube-controller-manager-k8smaster          1/1     Running   0          136m   192.168.136.174   k8smaster   <none>           <none>
kube-proxy-gbrdl                           1/1     Running   0          136m   192.168.136.174   k8smaster   <none>           <none>
kube-proxy-jqsc4                           1/1     Running   0          86m    192.168.136.175   worker      <none>           <none>
kube-scheduler-k8smaster                   1/1     Running   0          136m   192.168.136.174   k8smaster   <none>           <none>



#kubectl -n kube-system get nodes -o wide
NAME        STATUS   ROLES           AGE    VERSION    INTERNAL-IP       EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION       CONTAINER-RUNTIME
k8smaster   Ready    control-plane   137m   v1.29.15   192.168.136.174   <none>        Ubuntu 22.04.5 LTS   5.15.0-156-generic   cri-o://1.33.0
worker      Ready    <none>          88m    v1.29.15   192.168.136.175   <none>        Ubuntu 22.04.5 LTS   5.15.0-156-generic   cri-o://1.33.0



==================================================================================================================================================================








